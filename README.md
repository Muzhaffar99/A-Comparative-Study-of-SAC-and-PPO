# RL-Based Control of Reservoir Systems Using SAC and PPO

## Author

**Muzhaffar Maruf Ibrahim**

Department of Automatic Control & Systems Engineering, University of Sheffield

---

## üß† Abstract

This project presents a **comparative study** of two state-of-the-art **Reinforcement Learning (RL)** algorithms ‚Äî **Soft Actor-Critic (SAC)** and **Proximal Policy Optimization (PPO)** ‚Äî applied to **single-tank and quadruple-tank water level control systems**. The research benchmarks these RL methods against the conventional **PID controller**, using **MATLAB's Reinforcement Learning Toolbox** introduced in **R2024a**.

The controllers:

* Achieve optimal tracking of water level setpoints
* Minimize overshoot and steady-state error
* Operate effectively in nonlinear and dynamic environments
* Are trained using Simulink-integrated environments

---

## üßæ Problem Statement

This study focuses on nonlinear water tank systems, described by their dynamics:

* **Single-Tank System**
  Based on gravitational outflow and inflow rate control via voltage-regulated pumps.

* **Quadruple-Tank System**
  A MIMO system governed by coupled nonlinear equations with two pumps and multiple interacting tanks.

Control goals include:

* Achieving accurate water level tracking
* Minimizing rise time and overshoot
* Handling control saturation and nonlinear flow behavior

---

## üéØ Control Objective

* Evaluate SAC and PPO for trajectory tracking
* Compare with classical PID control
* Use shared metrics: rise time, overshoot, steady-state error, and reward
* Deploy Simulink environments for reproducibility
* Tune hyperparameters for real-time convergence and optimal performance

---

## ‚úÖ Algorithms Compared

| Algorithm | Type           | Policy        | Characteristics              |
| --------- | -------------- | ------------- | ---------------------------- |
| **PID**   | Classical      | Rule-based    | Fast but brittle             |
| **DDPG**  | RL, Off-Policy | Deterministic | Sensitive to noise           |
| **SAC**   | RL, Off-Policy | Stochastic    | High exploration & stability |
| **PPO**   | RL, On-Policy  | Stochastic    | Stable, high accuracy        |

---

## üîç Model Overview

### **Single-Tank System**

<p align="center">
  <img src="System Model.png" alt="Single Tank Diagram" width="400"/>
</p>

Governing equation:

$$
A \\frac{dH(t)}{dt} = bV - a\\sqrt{H}
$$

### **Quadruple-Tank System**

<p align="center">
  <img src="Quadruple-tank.png" alt="Quadruple Tank Diagram" width="400"/>
</p>

* Inputs: voltages $u_1, u_2$
* Outputs: water levels $h_1, h_2$
* Coupled nonlinear flow dynamics

---

## üìä Performance Metrics

| Controller | System         | Overshoot (%) | Ts (s)   | Reward   |
| ---------- | -------------- | ------------- | -------- | -------- |
| PID        | Single-Tank    | High          | Fast     | ‚Äî        |
| SAC        | Single-Tank    | Low           | Fast     | High     |
| PPO        | Single-Tank    | Very Low      | Slow     | Moderate |
| SAC        | Quadruple-Tank | Best Overall  | Fast     | High     |
| PPO        | Quadruple-Tank | Unstable      | Diverged | Low      |

---

## üìà Training Visualization

* **SAC Performance on Single-Tank:**

<p align="center">
  <img src="sac_training_single.png" alt="SAC Single-Tank" width="500"/>
</p>

* **PPO Performance on Quadruple-Tank:**

<p align="center">
  <img src="ppo_training_quad.png" alt="PPO Quadruple-Tank" width="500"/>
</p>

---

## üß∞ Implementation

MATLAB/Simulink environments were created for each case:

* `single_tank_pid.mdl` ‚Äî PID benchmark
* `single_tank_sac.mlx` ‚Äî SAC agent training
* `single_tank_ppo.mlx` ‚Äî PPO agent training
* `quadruple_tank_sac.mlx` ‚Äî Advanced RL on MIMO system
* `quadruple_tank_ppo.mlx` ‚Äî PPO comparison

---

## üìö References

> Key works include:
>
> * Sutton & Barto (2018) ‚Äì *Reinforcement Learning: An Introduction*
> * Haarnoja et al. (2018) ‚Äì *Soft Actor-Critic Algorithms*
> * Schulman et al. (2017) ‚Äì *Proximal Policy Optimization*
> * Rajesh Siraskar (2021) ‚Äì *Reinforcement Learning for Valve Control*

For full list, see the final report PDF.

---

## üì© Contact

*Muzhaffar Maruf Ibrahim*
[mmibrahim2@sheffield.ac.uk](mailto:mmibrahim2@sheffield.ac.uk)

---

Let me know if you'd like this exported as a `README.md` file or adapted for LaTeX.
